---
title: 'P&S-2022: Lab assignment 2'
author: "Ohinska, Shopska, Lukashenko"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with
    explanations + oral defense). Submission deadline **November 1,
    2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to
    **cms** both the source *R notebook* **and** the generated html
    file\
-   At the beginning of the notebook, provide a work-breakdown structure
    estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer
        to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is
        just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you
        use to complete the task) as well as histograms etc to
        illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding
        theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree
        with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit**
    ordinal number of your team on the list. Include the line
    **set.seed(team id number)** at the beginning of your code to make
    your calculations reproducible. Also observe that the answers **do**
    depend on this number!\
-   Take into account that not complying with these instructions may
    result in point deduction regardless of whether or not your
    implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it
    to a $7$-bit *codeword*
    $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where
    $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the
    received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome
    vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$
    *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary
    $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no.
    $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or
    more than one), while $(1 1 0 )$ means the third bit (or more than
    one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in
    $\mathbf{r}$ to get the corrected
    $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$
    G :=
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and
    find the estimate $\hat p$ of the probability $p^*$ of correct
    transmission of a single message $\mathbf{m}$. Comment why, for
    large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator
    of success by the standard error of your sample and using the CLT,
    predict the \emph{confidence} interval
    $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate
    $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while
    transmitting a $4$-digit binary message. Do you think it is one of
    the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
id <- 20

set.seed(id)
N = 10000
p <- id / 100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
  1, 0, 0, 1, 1, 0, 0,
  0, 1, 0, 1, 0, 1, 0,
  1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
  0, 1, 1, 0, 0, 1, 1,
  0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
 cat("The matrix G is: \n")
G
cat("The matrix H is: \n")
H
cat("The product GH must be zero: \n")
(G%*%H) %%2

encode_message <- function(m, G){
  as.vector((m %*% G) %% 2)
}

syndrome <- function(r) {
  as.vector((matrix(r, nrow=1) %*% H) %% 2)    #transform vector r (7-bit encoded word) into a matrix 1x7
}                                              #then multiplies matrix and H to get a syndrome. if z = (0, 0, 0),
                                               #then message is transmitted correctly, but if no, there will 1 on the position of a mistake

correction <- function(r) {
  z <- syndrome(r)         # calculate the syndrome
  pos <- z[1] + 2 * z[2] + 4 * z[3]    #define the position of the error
  if (pos != 0) {
    r[pos] <- (r[pos] + 1) %% 2  # change bit if incorrect
  }
  return(r)
}

decode <- function(r) {
  return(c(r[3], r[5], r[6], r[7]))     # to decode take only informational bits
}

success <- numeric(N)     # resulting vector
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0, 1), 4 * N, replace = TRUE), nrow = N)
}
messages <- message_generator(10000)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
errors <- matrix(rbinom(N * 7, 1, p), nrow = N)

received <- (codewords + errors) %% 2 # matrix of incorreclty recieved messages
```

The next steps include detecting the errors in the received messages,
correcting them, and then decoding the obtained messages. After this,
you can continue with calculating all the quantities of interest

```{r}
corrected <- t(apply(received, 1, correction))
decoded <- t(apply(corrected, 1, decode))

success <- apply(decoded == messages, 1, all)

hat_p <- mean(success) # empirical (real)
p_star <- (1 - p)^7 + 7 * p * (1 - p)^6 # theoretical
cat("Simulation hat_p =", hat_p, "\n")
cat("Theoretical p* =", p_star, "\n")

```

encode_message method codes our 4-byte message into 7-byte message
syndrome(r) method calculates the syndrome vector for checking errors
correction(r) finds and corrects olne-byte errors in coded word
decode(r) returns 4-byte message from corrected version of the word
After this we have a natrix of random messages and then them coded. Then
we generate message with errors with equal probability = p for all to
have errors. Later every received word goes through the function of
correction where one-byte mistakes get corrected and later decoded into
4-byte. Finally we calculate number of successes in transmitting words
with no mistakes hat_p - empirical probability of success, p_star -
theoretical probability of success.

For large N, the law of large numbers says that $\hat p$ \~ $p^*$,
because the fraction of successfull transmissions converges to the
expected success probability.

```{r}
N_new <- ceiling(1.96 * 1.96 * p_star * (1 - p_star)) / (0.03 * 0.03)
cat("Required N for epsilon <= 0.03:", N_new, "\n")
SE <- sqrt(hat_p * (1 - hat_p) / N_new)
eps <- 1.96 * SE # 1.96 is z-value for 95% probability
cat("Half-length of 95% confidence interval: ", eps, "\n")

```

Here we've got the standard error (shows how our estimation of p_hat can
differ from actual p*). Half of the interval shows how large the
distance from p\^ to p* can be with probability 95%. For this distance
to be less or equal to 0.03, we need N_new simulations. Larger number of
simulations gives more precise estimation of p\^ and narrows the
distance from theoretical value.

```{r}
num_errors <- rbinom(N, size = 4, prob = p)

hist(num_errors, breaks = -0.5:4.5, col = "skyblue",
     main = "Histogram of number of errors per 4-bit message",
     xlab = "Number of errors k", ylab = "Frequency")
```

The random variable that counts the number of wrong bits in a decoded
message has Binomial distribution, because it has a fixed number of
independent trials, every outcome is either success or loss, probability
of success is always the same. In our case, as we are 20th team, p =
0.2, so most part of the result will be distributed to the left, as 0.2
is not that high probability for the error.

**Do not forget to include several sentences summarizing your work and
the conclusions you have made!**

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a
Poisson distribution. As you remember, a Poisson random variable
describes occurrences of rare events, i.e., counts the number of
successes in a large number of independent random experiments. One of
the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big
*half-life period* $T$; it is vitally important to know the probability
that during a one second period, the number of nuclei decays will not
exceed some critical level $k$. This probability can easily be estimated
using the fact that, given the *activity* ${\lambda}$ of the element
(i.e., the probability that exactly one nucleus decays in one second)
and the number $N$ of atoms in the sample, the random number of decays
within a second is well modelled by Poisson distribution with parameter
$\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms
is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro
constant, and $M$ is the molar (atomic) mass of the element. The
activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is
measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive
element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life
period $T = 30.1$ years and mass
$m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by
$X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays
in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll
    need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$
    gets very close to a normal one as $n$ becomes large and identify
    that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical
        cumulative distribution function $\hat F_{\mathbf{s}}$ of
        $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$
        of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to
        visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.\
3.  Calculate the largest possible value of $n$, for which the total
    number of decays in one second is less than $8 \times 10^8$ with
    probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality,
        Chernoff bound and Central Limit Theorem, and compare the
        results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less
        than critical value ($8 \times 10^8$) and calculate the
        empirical probability; comment whether it is close to the
        desired level $0.95$

#### 1. Specify the parameter of the Poisson distribution of $X_i$

```{r}
M <- 137                      # atomic mass of Cesium-137
m <- id * 1e-6                # mass of each sample in grams
Na <- 6e23                    # Avogadro constant
N <- Na * m / M

T <- 30.1 * 365 * 24 * 3600   # half-life period in seconds
lambda <- log(2) / T

mu <- N * lambda
K <- 1e3
```

#### 2. Show that the distribution of the sample means of X1,...,Xn gets very close to a normal one as n becomes large and identify that normal distribution.

```{r}
for(n in c(5, 10, 50)) {
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
  
  mu_i <- mean(sample_means)
  sigma <- sqrt(mu_i/n)      # mu_i = Var(Xi)
  
  # ECDF
  xlims <- c(mu_i-3*sigma,mu_i+3*sigma)
  Fs <- ecdf(sample_means)
  plot(Fs,
       xlim = xlims,
       ylim = c(0,1),
       col = "blue",
       lwd = 2,
       main=paste("ECDF vs Normal CDF (n=", n, ")", sep=""))
  curve(pnorm(x, mean = mu_i, sd = sigma), col = "red", lwd = 2, add = TRUE)
  max_difference <- max(abs(Fs(sample_means) - pnorm(sample_means, mean = mu_i, sd = sqrt(mu_i/n))))
  cat("n =", n, "- Max difference =", max_difference, "\n")
}
```

#### 3. Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8*10^8$ with probability more than $0.95$.

```{r}
n <- 1
critical <- 8e8
prob_level <- 0.95

repeat {
  lambda_sum <- n * mu
  sums <- rpois(K, lambda = lambda_sum)

  emp_prob <- mean(sums < critical)
  
  cat("n =", n, "-> P(Sum < 8e8) =", emp_prob, "\n")
  
  if (emp_prob < prob_level) break
  n <- n + 1
}
cat("--------------------------------------------------\n")
n <- n -1
cat("The largest possible value of n is:", n, "\n")
```

```{r}
cat("--- Theoretical bounds for n ---\n")

# --- Initial constants ---
# Ensure that 'mu', 'critical', and 'prob_level' 
# were defined in previous code chunks.

prob_failure <- 1 - prob_level # 0.05

cat(paste("μ (mu) =", mu, "\n"))
cat(paste("C (critical) =", critical, "\n"))
cat(paste("P(failure) =", prob_failure, "\n"))

# --- 1. Markov's Inequality ---
# P(S_n >= C) <= E[S_n] / C
# We require that P(S_n >= C) <= 0.05
# (n * mu) / C <= 0.05
# n <= (0.05 * C) / mu

n_markov <- floor((prob_failure * critical) / mu)
cat("Markov Bound: n <=", n_markov, "\n")


# --- Define coefficients for the quadratic equation ---
# Both methods (CLT and Chernoff-Gaussian) reduce to solving
# the quadratic equation a*y^2 + b*y + c <= 0, where y = sqrt(n).
# a = mu
# c = -critical
# 'b' differs for each method.

a_quad <- mu
c_quad <- -critical


# --- 2. Central Limit Theorem (CLT) ---
# (mu)*y^2 + (q_clt*sqrt(mu))*y - C <= 0
# q_clt is the 0.95 quantile for N(0,1)

q_clt <- qnorm(prob_level) # approx 1.645
b_clt <- q_clt * sqrt(mu)

# Find the roots
roots_clt <- c(
  (-b_clt + sqrt(b_clt^2 - 4*a_quad*c_quad)) / (2*a_quad),
  (-b_clt - sqrt(b_clt^2 - 4*a_quad*c_quad)) / (2*a_quad)
)

# We need the positive root y
y_max_clt <- max(roots_clt)
n_clt <- floor(y_max_clt^2)
cat("CLT Bound: n <=", n_clt, "\n")


# --- 3. Chernoff Bound (for Gaussian tail) ---
# P(Z >= k) <= exp(-k^2 / 2)
# We seek k such that exp(-k^2 / 2) <= 0.05
# k >= sqrt(-2 * log(0.05))

q_chernoff <- sqrt(-2 * log(prob_failure)) # approx 2.448

# Use the same quadratic equation, but with q_chernoff
b_chernoff <- q_chernoff * sqrt(mu)

roots_chernoff <- c(
  (-b_chernoff + sqrt(b_chernoff^2 - 4*a_quad*c_quad)) / (2*a_quad),
  (-b_chernoff - sqrt(b_chernoff^2 - 4*a_quad*c_quad)) / (2*a_quad)
)

y_max_chernoff <- max(roots_chernoff)
n_chernoff <- floor(y_max_chernoff^2)
cat("Chernoff-Gaussian Bound: n <=", n_chernoff, "\n")
```

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of
    $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as
    $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the
        \textbf{r.v.} $X_i$ and calculate the sample mean
        $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the
        \emph{empirical cumulative distribution} function
        $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of
        $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph
        to visualize their proximity;\
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.
2.  The place can be considered safe when the number of clicks in one
    minute does not exceed $100$. It is known that the parameter $\nu$
    of the resulting exponential distribution is proportional to the
    number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where
    $\nu_1$ is the parameter for one sample. Determine the maximal
    number of radioactive samples that can be stored in that place so
    that, with probability $0.95$, the place is identified as safe. To
    do this,
    -   express the event of interest in terms of the \textbf{r.v.}
        $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov
        inequality, Chernoff bound and Central Limit Theorem and compare
        the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization
        $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum
        $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the
        $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe
        and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
team_id_number <- 20 

nu1 <- team_id_number + 10
K <- 10000
n_values <- c(5, 10, 50)
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))

```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 1 / nu1
max_differences <- data.frame(n = n_values, max_diff = NA)

```

#### We can now plot ecdf and cdf

```{r}
par(mfrow = c(2, 3)) 

cat("--- Running CLT Simulation ---\n")

for (i in 1:length(n_values)) {
  n <- n_values[i] 
  sample_means <- colMeans(matrix(rexp(n * K, rate = nu1), nrow = n))
  sigma <- 1 / (nu1 * sqrt(n))
  
  xlims <- c(mu - 4 * sigma, mu + 4 * sigma)

  hist(sample_means,
       probability = TRUE,
       breaks = 100,
       xlim = xlims,
       col = "lightblue",
       border = "grey",
       main = paste("Histogram of Sample Means, n =", n),
       xlab = "Sample Mean")
  
  # Add the theoretical normal DENSITY (PDF) curve
  curve(dnorm(x, mean = mu, sd = sigma), 
        col = "red", 
        lwd = 2, 
        add = TRUE)
  Fs <- ecdf(sample_means)
  
  plot(Fs, xlim = xlims, col = "blue", lwd = 2,
       main = paste("CDF Comparison, n =", n),
       xlab = "Sample Mean", ylab = "Cumulative Probability",
       verticals = FALSE, pch = NA)
  curve(pnorm(x, mean = mu, sd = sigma), 
        col = "red", lwd = 2, add = TRUE)

  ks_result <- ks.test(sample_means, "pnorm", mean = mu, sd = sigma)
  max_differences$max_diff[i] <- ks_result$statistic
  
  cat(paste("n =", n, " | Max Difference (K-S):", round(ks_result$statistic, 6), "\n"))
}
legend("bottomright", 
       legend = c("Empirical", "Theoretical"), 
       col = c("blue", "red"), lwd = 2, cex = 0.8)

par(mfrow = c(1, 1))

```

###Task 3.1: Justification and Conclusion Justification of our solution
Our solution is based on the Central Limit Theorem (CLT). The CLT states
that even if a distribution isn't normal (like our skewed Exponential
distribution), the distribution of its sample means will become normal
as the sample size n gets bigger.

Our original distribution is Exponential(nu1), where nu1 = 30.

The theoretical mean (mu) of this distribution is 1 / nu1 (or 1/30).

The theoretical variance is 1 / nu1\^2 (or 1/900).

The CLT predicts that the distribution of our sample means will be
approximately Normal.

The mean of this Normal distribution is mu = 1/30.

The variance is (Original Variance) / n, which is (1/900) / n.

The standard deviation (sigma) is the square root of this, which is 1 /
(30 \* sqrt(n)).

Our R code uses these exact theoretical parameters (mu \<- 1 / nu1 and
sigma \<- 1 / (nu1 \* sqrt(n))) to draw the red theoretical line. We
then compare our simulation results directly to this line to see if the
theorem holds.

Conclusion Our results strongly support the Central Limit Theorem.

From the Histograms: The plots clearly show the convergence. For n=5,
the histogram is still visibly skewed to the right. But as n increases
to 10 and 50, the shape becomes a much more symmetric, perfect bell
curve, matching the red theoretical line.

From the CDF Plots: The "S-curves" show the same thing. The blue
empirical line (from our simulation) gets closer and closer to the red
theoretical line (from the CLT) as n gets bigger.

From the Statistics: The console output gives numerical proof. The "Max
Difference" (the K-S statistic) measures the largest gap between the
blue and red curves. This difference decreased as n increased:

For n = 5, the difference was 0.059816

For n = 10, the difference was 0.053546

For n = 50, the difference was 0.024149

Our simulation with K=10,000 repetitions is reliable and clearly
demonstrates that the sample means converge to a normal distribution,
just as the CLT predicts. \### Task 3.2

```{r}
team_id_number <- 20
nu1 <- team_id_number + 10
K <- 10000                 
n <- 100                  

# --- (a) Problem Formulation ---
# Event of Interest: Find max integer N so that P(S_100 > 1) >= 0.95
# S_100 is the sum of 100 i.i.d. Exponential(nu) variables
# nu = nu1 * N = 30 * N
# E[S_100] = n / nu = 100 / nu
# Var[S_100] = n / nu^2 = 100 / nu^2

cat("=====================================================\n")
cat(" (b) Theoretical Bound Calculations\n")
cat("=====================================================\n\n")

# --- 1. Central Limit Theorem (CLT) Bound ---
cat("--- 1. Central Limit Theorem (CLT) Bound ---\n")
# We need P(Z > (nu - 100)/10) >= 0.95
# This means (nu - 100)/10 <= z(0.05)
z_score_05 <- qnorm(0.05) # This is -1.644854
nu_clt <- 10 * z_score_05 + 100 # 83.55146
N_clt <- nu_clt / nu1 # 2.785049
cat(paste("Maximal nu (CLT):", nu_clt, "\n"))
cat(paste("N <=", N_clt, "-> Max integer N =", floor(N_clt), "\n\n"))


# --- 2. Markov Inequality Bound ---
cat("--- 2. Markov Inequality Bound ---\n")
# We need 0.95 <= 100 / nu
nu_markov <- 100 / 0.95 # 105.2632
N_markov <- nu_markov / nu1 # 3.508772
cat(paste("Maximal nu (Markov):", nu_markov, "\n"))
cat(paste("N <=", N_markov, "-> Max integer N =", floor(N_markov), "\n\n"))


# --- 3. Chernoff Bound ---
cat("--- 3. Chernoff Bound ---\n")
# We need to solve: 100*log(nu/100) - nu <= -102.9957
# Let's test the value found in the analysis, nu = 77.4
nu_test <- 77.4
chernoff_value <- 100 * log(nu_test / 100) - nu_test # -103.0135
is_bound_met <- chernoff_value <= -102.9957 # TRUE
N_chernoff <- nu_test / nu1 # 2.58
cat(paste("Testing nu =", nu_test, "\n"))
cat(paste("Result:", chernoff_value, "(which is <= -102.9957:", is_bound_met, ")\n"))
cat(paste("N <=", N_chernoff, "-> Max integer N =", floor(N_chernoff), "\n\n"))


# --- 4. Comparison of Results ---
cat("--- Comparison of Results ---\n")
cat(paste("CLT: N =", floor(N_clt), "\n"))
cat(paste("Chernoff: N =", floor(N_chernoff), "\n"))
cat(paste("Markov: N =", floor(N_markov), "\n"))
cat("CLT and Chernoff are tighter and agree.\n")
cat("Predicted Maximal N = 2\n\n")


cat("=====================================================\n")
cat(" (c, d, e) Simulation to Verify Bounds\n")
cat("=====================================================\n")

# --- Simulation (Parts c, d, e) ---
N_values <- c(2, 3) 

par(mfrow = c(1, 2))


for (N in N_values) {
  
  nu <- N * nu1
  
  cat(paste("\n--- Simulating for N =", N, "(nu =", nu, ") ---\n"))

  # (c), (d) 
  s_vector <- rowSums(matrix(rexp(K * n, rate = nu), nrow = K))
  
  # (e) 
  safe_events <- sum(s_vector > 1)
  prob_safe <- safe_events / K
  
  cat(paste("Estimated 'safe' probability:", prob_safe, "\n"))
  cat(paste("Is", prob_safe, ">= 0.95 ?", prob_safe >= 0.95, "\n"))

  mu_sum <- n / nu
  sigma_sum <- sqrt(n) / nu
  
  hist(s_vector,
       probability = TRUE,
       breaks = 100,
       col = "lightblue",
       border = "grey",
       main = paste("Distribution of S_100 for N =", N, "(nu =", nu, ")"),
       xlab = "Time until 100th click (minutes)")

  curve(dnorm(x, mean = mu_sum, sd = sigma_sum), 
        col = "red", 
        lwd = 2, 
        add = TRUE)
  
  abline(v = 1, col = "purple", lwd = 3, lty = 2)
  
  legend("topright", 
         legend = c("Simulated Sums", "CLT Approx.", "Safe Threshold (1 min)"), 
         col = c("lightblue", "red", "purple"), 
         lwd = c(NA, 2, 3), 
         lty = c(NA, 1, 2), 
         pch = c(22, NA, NA),
         pt.bg = "lightblue",
         cex = 0.8)
}

par(mfrow = c(1, 1))

```

###Task 3.2: Justification and Conclusion Justification of our solution
The goal is to find the max integer N (number of samples) so that the
probability of being "safe" is at least 0.95.

Formulation: The "safe" event is "fewer than 100 clicks in 1 minute". As
hinted, this is equivalent to the time for 100 clicks, S_100, being
greater than 1 minute. So, we need to find N such that P(S_100 \> 1) \>=
0.95.

Parameters: S_100 is the sum of 100 variables from an Exponential(nu)
distribution, where nu = N \* nu1 = N \* 30.

Justification (CLT): Since n=100 is large, we used the Central Limit
Theorem to approximate the distribution of the sum S_100 as a Normal
distribution.

The mean of this sum is E[S_100] = n / nu = 100 / (30\*N).

The variance of this sum is Var(S_100) = n / nu\^2 = 100 / (30\*N)\^2.

Theoretical Bound: We solved P(S_100 \> 1) \>= 0.95 for N. This
standardizes to finding a Z-score: P(Z \> (1 - Mean) / SD) \>= 0.95.
This simplifies to P(Z \> (30*N - 100) / 10) \>= 0.95. For this to be
true, the Z-score (30*N - 100) / 10 must be less than or equal to
-1.645. Solving for N, we got N \<= 2.785.

The maximal integer N is therefore 2. Our simulation was designed to
test N=2 (which we predicted is safe) and N=3 (which we predicted is
unsafe).

Conclusion Our theoretical bound of N=2 was confirmed by the simulation.

For N = 2 (nu = 60): The simulation showed an Estimated 'safe'
probability of 1.0 (or 100%). This is \>= 0.95, so N=2 is safe. The
histogram for N=2 shows why: the distribution is centered at 1.67
minutes, and the "unsafe" line at 1.0 minute is far in the left tail
(about 4 standard deviations away). It was expected that none of our
10,000 simulations would fail.

For N = 3 (nu = 90): The simulation showed an Estimated 'safe'
probability of 0.8378 (or 83.78%). This is \< 0.95, so N=3 is unsafe.
The histogram for N=3 shows the distribution is centered at 1.11
minutes, much closer to the "unsafe" line at 1.0. A large part of the
distribution is on the wrong side of the line, which is why the
probability is much lower.

Our simulation with K=10,000 repetitions is reliable and confirms the
CLT-based calculation. The maximal number of radioactive samples that
can be safely stored is N=2.

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its
    moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why
        $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$
        and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations
        $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of
        $Y := \frac{1}{X}$ to calculate the values of
        $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$.
        Comment on the received results;

```{r}
mu <- 20
sigma <- sqrt(47)
N <- 100

X <- rnorm(N, mean = mu, sd = sigma)

Y <- 1 / X

X_bar <- mean(X)
Y_bar <- mean(Y)
one_over_X_bar <- 1 / X_bar

cat("X_bar:", X_bar, "\n")
cat("1/X̄:", one_over_X_bar, "\n")
cat("Ȳ", Y_bar, "\n")
cat("Difference (Ȳ - 1/X̄):", Y_bar - one_over_X_bar, "\n")

```

X_bar is very close to the true mean, no matter how many times we run
the code. It tells us about correctness of the simulation. Ȳ and 1/X are
different because of non-linearity of the transformation Jensen's
Inequality states that for a random variable X and a convex function
f(x) (in our case 1/X), the following holds:
$\mathbb{E}(\frac{1}{X}) > \frac{1}{\mathbb{E(X)}}$;

```         
3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?
```

```{r}
set.seed(20)
lambda <- 2
X <- rexp(N, rate = lambda)
Y <- rexp(N, rate = lambda)

Z <- log(X) + 5

qqplot(X, Y, main = "Quantile-quantile plot of X and Y", xlab = "Quantiles of X", ylab = "Quantiles of Y")
plot(X, Y, main="Scatterplot of X and Y", xlab = "X", ylab = "Y")

```

Scatterplot of X and Y is chaotic and dispersed because each point x_i,
y_i represents a point generated independently of one another. Since
variables X and Y do not provide any information about one another,
there is no relation between them. This results in a chaotic and
dispersed scatterplot. In contrast, Q-Q plot does not examine the
relation between individual observations, but rather compares whole
distributions. Before constructing a qq plot, both samples are sorted
and corresponding quantiles are plotted. As X and Y follow the same
theoretical distribution, their quantiles coincide and increase
proportionally, so points align approximately along a straight line.

```{r}
set.seed(20)
lambda <- 2
X <- rexp(N, rate = lambda)
Y <- rexp(N, rate = lambda)

Z <- log(X) + 5

qqplot(X, Z, main = "Quantile-quantile plot of X and Z", xlab = "Quantiles of X", ylab = "Quantiles of Z")
plot(X, Z, main="Scatterplot of X and Z", xlab = "X", ylab = "Z")
```

Scatterplot of X and Y forms a smooth, increasing curve because Z is a
deterministic transformation of X. This smooth structure indicates a
strong relation between two variables. In contrast, Q-Q plot deviates
from straight line because the logarihtmic transformation of changes the
shape of the distribution, while X is exponentially distributed, Z
becomes approximately symmetric and shifted. So q-q plot reveals that X
and Z have different distributions.

Dependent pairs: X and Z Independent pairs: X and Y We can observe their
dependecies on plot above. X and Y are similar because they have the
same distribution.

```         
------------------------------------------------------------------------
```

2.  You toss a fair coin three times and a random variable $X$ records
    how many times the coin shows Heads. You convince your friend that
    they should play a game with the following payoff: every round
    (equivalent to three coin tosses) will cost £$1$. They will receive
    £$0.5$ for every coin showing Heads. What is the expected value and
    the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X: It is **Binomially
        distributed**

    Explanation:

    -   Each coin toss is an independent Bernoulli trial with two
        outcomes: Head (success) or Tail (failure).

    -   The probability of success (Head) is p = 0.5. - There are n = 3
        independent trials.

    So $X \sim \text{Binomial}(n = 3, p = 0.5)$

3.  What are the expected value and variance of X? Simulate realizations
    $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample
    mean $\overline{\mathbf{X}}$ and sample variance
    $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$.
    Comment on the results;

    ```{r}
    # Theoretical
    # E(X) = np = 1.5
    # Var(X) = np(1 - p) = 0.75

    # Simulation
    n_sim <- 100
    n_trials <- 3
    p_success <- 0.5

    x <- rbinom(n_sim, n_trials, p_success)

    x_bar <- mean(x)
    s2_x <- var(x)

    # Comment on results
    # The sample mean and sample variance (from the simulation) are
    # close to the theoretical values. They will not be identical due
    # to sample randomness, with small number of simulations (n = 100)

    cat("--- Results for X ---\n")
    cat("Theoretical E[X]:", 1.5, "\n")
    cat("Sample Mean (x_bar):", x_bar, "\n\n")

    cat("Theoretical Var(X):", 0.75, "\n")
    cat("Sample Variance (s2_x):", s2_x, "\n")
    ```

4.  What are the expected value and variance of Y? Simulate realizations
    $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample
    mean $\overline{\mathbf{Y}}$ and sample variance
    $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$.
    Comment on the results;

    ```{r}
    # Theoretical
    # E(X) = 0.5 * np - 1 = -0.25
    # Var(X) = 0.5 ^ 2 * np(1 - p) = 0.1875

    # Simulation
    y <- 0.5 * x - 1

    y_bar <- mean(y)
    s2_y <- var(y)

    # Comment on results
    # Negative expected value E[Y]=−0.25 means the game is unfavorable for the friend.
    # On average, the friend will lose £0.25 each time they play this game

    cat("\n--- Results for Y ---\n")
    cat("Theoretical E[Y]:", -0.25, "\n")
    cat("Sample Mean (y_bar):", y_bar, "\n\n")

    cat("Theoretical Var(Y):", 0.1875, "\n")
    cat("Sample Variance (s2_y):", s2_y, "\n")
    ```

**Do not forget to include several sentences summarizing your work and
the conclusions you have made!**

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what
difficulties you had etc.
